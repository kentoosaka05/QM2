{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kentoosaka05/QM2/blob/main/W7_Natural_Language_Processing_Student_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxouC6cfjAYL"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "## *Workshop 4*  [![Open In Colab](https://github.com/oballinger/QM2/blob/main/colab-badge.png?raw=1)](https://colab.research.google.com/github/oballinger/QM2/blob/main/notebooks/W04.%20Natural%20Language%20Processing.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sLO5n6tnemE"
      },
      "source": [
        "## Background\n",
        "\n",
        "Exxon Mobil is the 4th largest oil company in the world. In 1978, an Exxon scientist named James Black wrote an [internal briefing](https://insideclimatenews.org/documents/james-black-1977-presentation/) called \"The Greenhouse Effect\" in which he warned: “Present thinking holds that man has a time window of five to ten years before the need for hard decisions regarding changes in energy strategies might become critical.”\n",
        "\n",
        "Rather than acting on this information, Exxon spent the next [forty years aggressively funding climate denial](https://news.harvard.edu/gazette/story/2021/09/oil-companies-discourage-climate-action-study-says/). Recently, [a U.S. court ruled](https://www.theguardian.com/environment/2022/may/24/exxon-trial-climate-crimes-fossil-fuels-global-heating) that ExxonMobil must face trial over accusations that it lied about the climate crisis and covered up the fossil fuel industry’s role in worsening environmental devastation.\n",
        "\n",
        "### Earnings Calls\n",
        "Every three months, Exxon conducts an [\"earnings call\"](https://www.investopedia.com/terms/e/earnings-call.asp); a conference call between the management of a public company, analysts, investors, and the media to discuss the company’s financial results during a given reporting period, such as a quarter or a fiscal year.\n",
        "\n",
        "You can [register](https://globalmeet.webcasts.com/starthere.jsp?ei=1488251&tp_key=440e363aaf) to attend their next one if you want! No worries if you miss it, they provide [transcripts](https://corporate.exxonmobil.com/Investors/Investor-relations/Investor-materials-archive#Quarterlyearningsmaterials) on their website.\n",
        "\n",
        "These transcripts provide an intimate window into the company's dealings. We can see how much pressure investors are putting on the company to tackle climate change, and how the company responds.\n",
        "\n",
        "We'll be working with transcripts spanning nealry 20 years and over 10 million words; that's like reading the Harry Potter series 10 times. Then, we'll look at a sample of 100,000 tweets that use the #ExxonKnew hashtag, and analyze public pressure on the company."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIhVuwMlbvQC"
      },
      "source": [
        "![](https://haha.business/business.jpg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZwtwz53jAYM"
      },
      "source": [
        "## Downloading the Data\n",
        "Let's grab the data we will need this week from our course website and save it into our data folder. If you've not already created a data folder then do so using the following command.\n",
        "\n",
        "Don't worry if it generates an error, that means you've already got a data folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8OXlIf2jAYN"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install spacy\n",
        "!pip install scattertext\n",
        "!pip install tika\n",
        "!pip install spacytextblob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeYFCcv4jAYT"
      },
      "outputs": [],
      "source": [
        "#Make a ./data/wk4 directory\n",
        "!mkdir data\n",
        "!mkdir data/wk4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbImGRYTq3D9"
      },
      "outputs": [],
      "source": [
        "!curl https://storage.googleapis.com/qm2/wk4/Exxon.json -o data/wk4/Exxon.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXB0zCQEjAYY"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import json\n",
        "import pylab\n",
        "from IPython.core.display import display, HTML\n",
        "import nltk\n",
        "from tika import parser\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob\n",
        "\n",
        "%matplotlib inline\n",
        "pylab.rcParams['figure.figsize'] = (10., 8.)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe('spacytextblob')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU1g5loJ2_3s"
      },
      "source": [
        "## Downloading and reading one earnings call\n",
        "\n",
        "Exxon host earnings calls on their website in PDF form. Usually, working with PDFs is a real pain as they are not machine-readable. Using a python package called [tika](https://www.geeksforgeeks.org/parsing-pdfs-in-python-with-tika/), we can \"parse\" a pdf, turning it into machine-readable text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibXsDF1uW_dP"
      },
      "outputs": [],
      "source": [
        "# define the URL where your PDF lives. You could also upload your own pdf.\n",
        "#url='https://corporate.exxonmobil.com/-/media/Global/Files/investor-relations/quarterly-earnings/earnings-transcripts/2022-earnings-transcripts/1Q22-XOM-Earnings-Call-Transcript-4-29-22.pdf'\n",
        "url='https://d1io3yog0oux5.cloudfront.net/_74d009918ead0ec6acdd6bbaf27a8316/exxonmobil/db/2288/22123/earnings_release/XOM+2Q23+Earnings+Press+Release+Website.pdf'\n",
        "# parse the pdf by feeding tika the URL and store the text in an object called \"raw\"\n",
        "raw = parser.from_file(url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4Wevbuz4Q8i"
      },
      "source": [
        "Now, we have an object called \"raw\" that contains some useful information. Notice the squiggly brackets; this is a dictionary. It contains several fields, including some useful metadata such as the author"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1na8WczX4RQm"
      },
      "outputs": [],
      "source": [
        "date=raw['metadata']['dcterms:created']\n",
        "title=raw['metadata']['dc:title']\n",
        "raw_text=raw['content']\n",
        "\n",
        "print('Date: ', date)\n",
        "print('Title: ', title)\n",
        "print('Word Count: ', len(raw_text))\n",
        "print('Text:')\n",
        "raw_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPC9xxqr8dji"
      },
      "source": [
        "look at that! we're beginning to give some structure to our text data. But suppose I wanted to analyze multiple earnings calls; I need to organize this data so that it can accomodate new entries. As always, we want to **tabularize** our data. Let's create a dataframe with three columns (Date, Title, and Text) in which each row is one earnings call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgbT-rjW7-Y4"
      },
      "outputs": [],
      "source": [
        "# create a dataframe using the above data\n",
        "call=pd.DataFrame({'Date':[date],'Title':[title],'Text':[raw_text]})\n",
        "\n",
        "# remember, datetime information almost always reaches us as text.\n",
        "# we need to explicitly convert it to the datetime data type.\n",
        "call['Date']=pd.to_datetime(call['Date'], infer_datetime_format=True)\n",
        "\n",
        "# Let's see what we've got.\n",
        "call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br3d1ibYBQYc"
      },
      "source": [
        "Now, if we were so inclined, we could use a loop to repeat this process for a large number of earnings calls, yielding a neatly organized dataframe containing the date, title, and text of earnings calls over time. I've done this so you don't have to, and stored it as a file called \"Exxon.json\". It spans 2002-2019, and contains over 10 million words' worth of earnings calls. Let's take a peek:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAcADSi_BRUX"
      },
      "outputs": [],
      "source": [
        "df=pd.read_json('data/wk4/Exxon.json')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsEn8Sq_BqVu"
      },
      "source": [
        "Great-- we've got a structured dataset of earnings calls. But even though the data has *structure*, the data in the \"Text\" column still needs some cleaning and processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELfPVOKy6jh7"
      },
      "source": [
        "## Dirty Words\n",
        "\n",
        "Text often comes 'unclean' either containing tags such as HTML (or XML), or has other issues.\n",
        "We've already done a bit of tidying, but it's been relatively straightforward. Be cautious when committing to a text analysis project - you may spend a great deal of time tidying up your text.\n",
        "\n",
        "For example, you may have noticed \"\\n\\n\\n\\n\\n\\n\\n\\n...\" in the text of the first earnings call we downloaded. This is a character (just like \"a\" or \"$\") except it indicates that we want to create a new line. It's part of the formatting of the pdf. That's not really useful information to us. Let start by selecting an earnings call; i've chosen the 38th in this dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lliAXrWoCXsD"
      },
      "outputs": [],
      "source": [
        "call=df.iloc[38]\n",
        "\n",
        "print('Date: ', call['Date'])\n",
        "print('Title: ', call['Title'])\n",
        "print('Word Count: ', len(call['Text']))\n",
        "print('Text:')\n",
        "call['Text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg_ohnHgC_3N"
      },
      "source": [
        "This call took place on May 25th, 2016. The transcript is over 125,000 words, nearly as long as the third Lord of the Rings book. It would be a pain to read all of it, so we'll use python to extract insights. Currently, the contents of `call[\"Text\"]` is a [\"string\"](https://docs.python.org/3/library/stdtypes.html#text-sequence-type-str)-- a sequence of characters. We can do a number of things with strings, including splitting a big string into smaller strings using a specific delimiter and the `.split()` function. For example, I can break down the whole text of the earnings call roughly into sentences by splitting the string every time I encounter a period (\".\"). This returns a list of smaller strings, and if i select the first one using `[0]`, I get the first sentence of this call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lt9pXgugv6L"
      },
      "outputs": [],
      "source": [
        "call['Text'].split('.')[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnNtpHY8iWvv"
      },
      "source": [
        "Lovely! the first sentence is an introduction by then-CEO [Rex Tillerson](https://en.wikipedia.org/wiki/Rex_Tillerson).\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Rex_Tillerson_official_portrait.jpg/800px-Rex_Tillerson_official_portrait.jpg.jpg\" alt=\"drawing\" width=\"200\"/>\n",
        "\n",
        "He was CEO of Exxon from 2006 until he retired on January 1st 2017. One month later, he was sworn in as U.S. Secretary of State under Donald Trump. Let's see what Rex thinks about climate change!\n",
        "\n",
        "## Regular Expressions (Regex)\n",
        "\n",
        "Another thing we can do with strings in python is search them using regular expressions. A regular expression is a sequence of characters that specifies a search pattern in text. You can play around building some regex queries using this [tool](https://regexr.com/).\n",
        "\n",
        "You can think about this as Ctrl+F on steroids; In its simplest form, we can use regex to search for a character, word, or phrase in a bunch of text. For example, we can use regular expressions to count how many times \"climate change\" is mentioned in this earnings call using the `re.findall()` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMtXvNgVgtMc"
      },
      "outputs": [],
      "source": [
        "# import the regular expressions library\n",
        "import re\n",
        "\n",
        "# use the findall function to search for mentions of \"climate change\" in the text of our call\n",
        "climate_change = re.findall(r'climate change', call['Text'], re.IGNORECASE)\n",
        "\n",
        "# this returns a list of strings matching our search term.\n",
        "# the length of the list gives us the number of occurances\n",
        "len(climate_change)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIC8_1ipDV3P"
      },
      "source": [
        "Looks like climate change is mentioned 51 times in this earnings call.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Exercise\n",
        "\n",
        "how many times is the phrase \"global warming\" mentioned?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "But we have 182 earnings calls in this sample-- suppose we want to count the number of times climate change is mentioned in each one, so we can see the salience of this topic over time.\n",
        "\n",
        "### Applying a lambda function to a dataframe\n",
        "\n",
        "Because each row of our dataframe `df` is an earnings call (the text of which is contained in `df['Text']`, we want to apply the analysis we did for the single earnings call above to each row of `df`.\n",
        "\n",
        "We can accomplish this using a [**lambda function**](https://www.w3schools.com/python/python_lambda.asp). This allows us to iterate over each value in a dataframe column, and apply a function to it. In the simple example below, I use a lambda function to create a new column that is takes the values from a different column and multiplies them by 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRZxy6e9-8VH"
      },
      "outputs": [],
      "source": [
        "# create a dataframe called \"example\" with one column called \"numbers\" which contains numbers 0-5\n",
        "example= pd.DataFrame({'numbers':[0,1,2,3,4,5]})\n",
        "\n",
        "# print the dataframe\n",
        "print(\"\\n Before applying lambda function: \\n\", example)\n",
        "\n",
        "# create a new column called \"doubled numbers\"\n",
        "# apply a lambda function that iterates over each row in the \"numbers\" column\n",
        "# call each row \"x\", and multiply it by 2\n",
        "example['doubled numbers']= example['numbers'].apply(lambda x: x*2)\n",
        "\n",
        "# print the dataframe, which now contains the new column\n",
        "print(\"\\n \\n After applying lambda function: \\n\", example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3iVF369Bewr"
      },
      "source": [
        "There were simpler ways of doing this (namely, `example[doubled numbers]=example['numbers']*2`). But if we want to do something more complex, lambda functions are very useful. Remember, we used `re.findall(r'climate change', call['Text'], re.IGNORECASE)` to get a list of mentions of climate change in the text of one earnings call, and measured the length of the list using `len()` to count the number of mentions. We can turn this into a lambda function as follows:\n",
        "\n",
        "`df['Text'].apply(lambda x: len(re.findall(r'climate change', x, re.IGNORECASE)))`\n",
        "\n",
        "1. `df['Text']`: the column we want to iterate over.\n",
        "2. `.apply(lambda x:`: iterate over each row in the column, and call each value in that column x. In other words, x will represent the text of each earnings call.\n",
        "3. `len(re.findall(r'climate change', x, re.IGNORECASE)` this is exactly the same as what did previously to find the number of mentions of climate change in the one earnings call, except that we swapped `call['Text']` with `x`, since we want to do this for the text of every earnings call."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NkYM01uAba2"
      },
      "outputs": [],
      "source": [
        "# create a column called \"climate change\" that contains the count of mentions of this keyword\n",
        "df['climate change']=df['Text'].apply(lambda x: len(re.findall(r'climate change', x, re.IGNORECASE)))\n",
        "\n",
        "# print the title of each earnings call, along with the number of mentions of climate change.\n",
        "print(df[['Title','climate change']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQDrl8osED4V"
      },
      "source": [
        "Amazing! We've now got a column indicating how many times \"climate change\" was mentioned in each earnings call.  \n",
        "\n",
        "---\n",
        "\n",
        "### Exercise\n",
        "\n",
        "Create three new columns that count the frequency of the terms \"global warming\", \"carbon capture\", and another phrase or word of your choosing. When you've done this, edit the code below so that it not only shows the frequency of \"climate change\" mentions, but also the three additional columns you created.\n",
        "\n",
        "Advanced: Our current measure of the number of mentions of keywords might be biased: if one earnings call mentions climate change 10 times more than another, but that earnings call has 10 times more words, then the *rate* of keyword mentions hasn't actually increased; people are just talking more. You can get the word count of each earnings call in the lambda function above using `len(x)` (using `len()` on a string will get you a word count). Edit the lambda function above such that we don't get a *count* of the number of mentions of climate change, but the *rate* of mentions (i.e., count of \"climate change\" divided by total word count per call).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's plot the frequency of these mentions over time to analyze temporal trends in the salience of climate change and other keywords in these calls. We'll accomplish this using"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg2y71xgBucr"
      },
      "outputs": [],
      "source": [
        "# extract the year from the date column\n",
        "df['Year']=df['Date'].dt.year\n",
        "\n",
        "# group the dataframe by year, calculating the sum of the \"climate change\" column\n",
        "# save it as a new dataframe called \"yearly\"\n",
        "yearly=df.groupby('Year')['climate change'].sum()\n",
        "\n",
        "# plot yearly\n",
        "yearly.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tntj7KH03bdo"
      },
      "source": [
        "Do you notice any patterns in the salience of these topics over time?\n",
        "\n",
        "## Intermediate Regex\n",
        "\n",
        "Great. We can see how frequently climate related keywords come up in earnings calls between shareholders and Exxon Mobil representatives over time. But what if we want to look at what they're actually saying?\n",
        "\n",
        "We can get a bit fancier with Regex to look at the content of these discussions. Regex can be pretty confusing, but it's also a very powerful tool. Before moving on, let's familiarize ourselves a bit more with regex.\n",
        "\n",
        "Let's try to extract all *sentences* containing the phrase \"climate change\"; the regex would look like this:\n",
        "\n",
        "`([^.]*climate change[^.]*)`\n",
        "\n",
        "1. `()` indicates that we want to match a group of characters, not just the characters themselves. In this case, the group is a sentence, not just the word climate change. But how do we\n",
        "2. `[^.]*` we want to match all characters except periods. This will break the text up into sentences\n",
        "3. `climate change` the phrase we want our sentence to contain.\n",
        "\n",
        "when you put it all together, the regex will search for groups of characters (1.) bounded by periods (2.) that contain the phrase \"climate change\" (3.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK9uSqT15VSG"
      },
      "outputs": [],
      "source": [
        "# create a list called \"climate_sentences\" that contains the results of this query\n",
        "climate_sentences=re.findall(r\"([^.]*climate change[^.]*)\",\" \".join(df['Text']))\n",
        "\n",
        "print(len(climate_sentences))\n",
        "# print the first 10 sentences in the list\n",
        "for sentence in climate_sentences[:10]:\n",
        "  print('\\n', sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXs2yT48zdCN"
      },
      "source": [
        "### Semantic Analysis\n",
        "\n",
        "Now we can see the *sentences* which mention climate change, which helps us understand a bit about the context. We can perform semantic analysis on some of these sentences to take a close look at the grammar of some of these sentences; I've isolated the 9th sentence and produced a dependency tree, like the ones we've seen in class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rY_a3fJy__7"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "#run the NLP pipeline on the 9th sentence from our list of sentences about climate change.\n",
        "doc = nlp(climate_sentences[8].lstrip())\n",
        "\n",
        "#print out the dependency tree\n",
        "displacy.render(doc, jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0kCX-UNj1QU"
      },
      "source": [
        "The root of this dependency tree is the verb \"describes\". The main subject is the Economist, and the object is Exxon. But we're still missing one vital piece of information: who is speaking? It makes a big difference to our understanding of whats going on. Are mentions of climate change increasing over time because shareholders are asking more questions? Or did CEO Rex Tillerson have a spiritual awakening in which all he wants to do is talk about climate change? For that, we need to figure out who's talking, and resturcture our dataframe.\n",
        "\n",
        "## Advanced Regex\n",
        "\n",
        "The earnings call transcript is structured in such a way that it should be possible to separate speakers based on regular expressions. Every time a new person is speaking, they are introduced in the transcript in a new paragraph; Consider the excerpt below:\n",
        "\n",
        "```\n",
        "OPERATOR: Our next question comes from Philip Weiss with Argus Research.\n",
        "\n",
        "PHILIP WEISS, ANALYST, ARGUS RESEARCH COMPANY: Good morning. I did have one, most of my questions have been answered, but I do have one follow-up on the US. You said that the rig count that's being used for liquids-rich is rising but when I look at production, natural gas as a percentage of your total production has grown, and liquids has actually fallen a little bit. So, I wonder if you can just comment on when we might start to see that trend change?\n",
        "\n",
        "DAVID ROSENTHAL: Sure. The fall off in the liquids is really just the overall decline in the conventional, as well as some divestments. You'll recall we had a divestment in the Eastern Gulf of Mexico and that had an impact on us year-over-year in particularly in the second half.\n",
        "In terms of when we'll see significant production growth out of the unconventional, I mentioned some of the increases in percentages, although we haven't given all of the specific production volumes, but we'll do that as we progress.\n",
        "```\n",
        "\n",
        "Now, we can't simply split by new line (`\\n`); David Rosenthal has two paragraphs. We also can't just split using `:`, since this may appear in the text other than to indicate speakers. Let's describe the features of the characters we're looking to split out:\n",
        "\n",
        "`([A-Z]+.+[A-Z]+:)`\n",
        "\n",
        "  1. It's a group of characters\n",
        "    * regex: `()`\n",
        "  2. The words are all caps, and can contain any characters\n",
        "    * regex:`([A-Z])`\n",
        "  2. There can be multiple words, and they can be separated by anything\n",
        "    * regex: `([A-Z]+.+[A-Z])`\n",
        "  4. The sequence always ends in a colon\n",
        "    * regex: `([A-Z]+.+[A-Z]+:)`\n",
        "\n",
        "Let's use this regex in `re.findall()` to get a list of the speakers on this call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkBQe55wlrac"
      },
      "outputs": [],
      "source": [
        "# create a list of all the speakers by searching text of the earnings call for the above regex.\n",
        "speakers = re.findall(r'([A-Z]+.+[A-Z]+: )', call['Text'])\n",
        "\n",
        "# because they don't introduce the speaker in the opening statement, insert a placeholder at the beginning of this list.\n",
        "speakers.insert(0,'INTRODUCTION')\n",
        "\n",
        "# using set(list) will give you the unique values in a list\n",
        "# the length of set(list) gives us the number of unique speakers\n",
        "print('There are', len(set(speakers)),'speakers on this call:')\n",
        "\n",
        "# let's print out the first 10 speakers:\n",
        "for speaker in speakers[:10]:\n",
        "  print(speaker)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcoB9FRul1j7"
      },
      "source": [
        "We want to do more than just identify the speakers though; we want to break up the text of our earnings call into chunks of speech and associate each chunk of speech with its speaker. We can split a string using regular expressions using `re.split(<regex>,<text>)`. This takes one block of text, splits it into chunks using the regex, and returns a list of chunks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GXQv50Fl2Tg"
      },
      "outputs": [],
      "source": [
        "# split the text of the earnings call using our regex, save the list as \"speech\"\n",
        "speech=re.split(r'[A-Z]+.+[A-Z]+: ', call['Text'])\n",
        "\n",
        "# now, we can print the fourth speaker:\n",
        "print('Speaker: \\n', speakers[3])\n",
        "\n",
        "# and the text of the fourth speech:\n",
        "print('\\n Speech: \\n', speech[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHtSeIzlOoAf"
      },
      "source": [
        "Now we've associated chunk of speech with their speaker, amazing. Let's create a new dataframe that reflects this structure. Currently, our dataframe `call` has one row. Let's use the two lists we just created, `speakers` and `speech`, to create a dataframe in which each row is one chunk of speech. A column called \"speaker\" will indicate who is speaking, and a column called \"speech\" will contain the text of the speech:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOcM-AEUAkB_"
      },
      "outputs": [],
      "source": [
        "# create the new dataframe, from the two lists, and name it \"speaker_df\"\n",
        "speaker_df=pd.DataFrame({\"speaker\":speakers,\"speech\":speech})\n",
        "\n",
        "# clean up the \"speaker\" column by removing the colons using .str.replace(\":\",\"\")\n",
        "# remove trailing white space using str.rstrip()\n",
        "speaker_df['speaker']=speaker_df['speaker'].str.replace(':','').str.rstrip()\n",
        "\n",
        "# print rows in which rex tillerson is speaking:\n",
        "print(speaker_df[speaker_df['speaker']==\"REX TILLERSON\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARwHu4b3hI5N"
      },
      "source": [
        "## Analyzing Distinguishing Terms\n",
        "\n",
        "And there we have it. We started with a PDF on a website, and we've ended up with a dataframe in which each row is a speech, with a column indicating who is speaking, what they're saying, and when they said it.\n",
        "\n",
        "Now, lets use this dataframe to create a scatterplot comparing the language used by the company's CEO Rex Tillerson and the company's shareholders.This will give us insights into the topics that are important for shareholders, and the debates that take place within the company.\n",
        "\n",
        "We'll do so using the [scattertext](https://spacy.io/universe/project/scattertext) libray:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej6_Xvfc-XnK"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import scattertext as st\n",
        "\n",
        "# create a corpus of text from the dataframe\n",
        "corpus = st.CorpusFromPandas(speaker_df, # load the dataframe\n",
        "                             category_col='speaker', # indicate which column contains the category we want to distinguish by\n",
        "                             text_col='speech', # indicate which column stores the text to be analyzed\n",
        "                             nlp=nlp).build() # load the NLP models used for analysis\n",
        "\n",
        "# remove stopwords from the corpus of text\n",
        "corpus=corpus.remove_terms(nlp.Defaults.stop_words, ignore_absences=True)\n",
        "\n",
        "# now, we create the scatterplot\n",
        "html = st.produce_scattertext_explorer(\n",
        "                   corpus, # load the corpus\n",
        "                   category=\"REX TILLERSON\", # indicate which category value we want to compare against all others; in this case, all rows in which \"REX TILLERSON\" is the speaker\n",
        "                   category_name='Rex Tillerson', # set the label on the plot as \"Rex Tillerson\"\n",
        "                   not_category_name='Others', # set the label on the plot for all other speakers as \"Others\"\n",
        "                   width_in_pixels=1000) #set the width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ0m_ST-jHDi"
      },
      "outputs": [],
      "source": [
        "# display the plot\n",
        "display(HTML(html))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm5dAf3Q1ZxS"
      },
      "source": [
        "The plot above compares the frequency of terms used by Exxon CEO Rex Tillerson (on the Y axis) against those used by other speakers (mainly shareholders, on the X axis). The top right corner will contain terms used frequently by both groups. the bottom left corner contains terms used infrequently by both groups. The top left corner contains terms used frequently by Rex Tillerson, but infrequently by shareholders. The bottom right corner contains terms used frequently by shareholders, but infrequently by Rex Tillerson.\n",
        "\n",
        "A list of top terms used by each group is shown in the right. On this list, we can see that \"climate change\" is the 4th most common phrase used by the \"Others\" category, but isn't even in the top ten for Rex. If you click on \"climate change\" in this list, it will give you some statistics on how frequently this term is used by each group, as well as a selection of example sentences in which the term appears. You can search for other words/phrases either by clicking on them in the scatterplot, or entering them into the \"Search the chart\" box below the scatterplot.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Exercise\n",
        "\n",
        "1. Search for \"ALEC\" in this plot, and then google the term to find out more about what this is. What are shareholders aiming to do regarding ALEC, and how does Tillerson respond?\n",
        "\n",
        "2. Search for the term \"carbon\". What differences do you notice in the use of this term by Rex Tillerson versus the shareholders?\n",
        "\n",
        "3. Use this plot to identify another topic that shareholders are pressuring Exxon about.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYiH6_iylDq5"
      },
      "source": [
        "# External pressure\n",
        "\n",
        "The fact that Exxon knew about climate change in the 1970s and still funded climate denial resulted in public outrage, culminating in an online movement organized around the twitter hashtag #ExxonKnew.\n",
        "\n",
        "I've downloaded almost 100,000 tweets containing the hashtag #ExxonKnew, between 2016 and 2017. Work together as a group to explore and analyze this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKuFJpv3myLh"
      },
      "outputs": [],
      "source": [
        "!curl https://storage.googleapis.com/qm2/wk4/Exxon_tweets_clean.csv -o data/wk4/Exxon_tweets_clean.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPVVRuZvnQ1r"
      },
      "outputs": [],
      "source": [
        "tweets=pd.read_csv('data/wk4/Exxon_tweets_clean.csv')\n",
        "tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyfjwIemrNyw"
      },
      "source": [
        "### Sentiment Analysis\n",
        "\n",
        "Sentiment analysis is the computational study of people's opinions, sentiments, emotions, appraisals, and attitudes towards entities such as products, services, organizations, individuals, issues, events, topics, and their attributes. Let's study the sentiment of the tweets in this dataset.\n",
        "\n",
        "[spacytextblob](https://spacytextblob.netlify.app/) performs sentiment analysis using the TextBlob library. Adding spacytextblob to a spaCy nlp pipeline creates a new extension attribute for the Doc.\n",
        "\n",
        "The `._.blob` attribute contains all of the methods and attributes that belong to the `textblob.TextBlob` class. Some of the common methods and attributes include:\n",
        "\n",
        "1. `._.blob.polarity`: a float within the range [-1.0, 1.0].\n",
        "2. `._.blob.subjectivity`: a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
        "3. `._.blob.sentiment_assessments.assessments`: a list of polarity and subjectivity scores for the assessed tokens.\n",
        "\n",
        "Let's run sentiment analysis on a single tweet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_2OFKQpqDpd"
      },
      "outputs": [],
      "source": [
        "# grab the 130th row in the dataframe and select the text of the tweet\n",
        "text=tweets.iloc[130]['text']\n",
        "\n",
        "# apply the NLP pipeline to this text.\n",
        "doc = nlp(text)\n",
        "\n",
        "print(text)\n",
        "print('Polarity: ', doc._.blob.polarity)\n",
        "print('Subjectivity: ', doc._.blob.subjectivity)\n",
        "print('Assessments: ', doc._.blob.sentiment_assessments.assessments)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIF7MLm-safJ"
      },
      "source": [
        "We can see that the model has deemed this tweet to be expressing negative sentiment: it has a polarity of -0.15. It also deems this to be a pretty subjective tweet, with a subjectivity score of 0.8. It does indeed appear to be expressing a subjective opinion. Finally, we can see which words are leading to this assessment. The word \"good\" is leading to a 0.7 increase in the polarity score, and a 0.6 increase in the subjectivity score. The word \"worst\" is leading to a -1 change polarity, and a +1 change in subjectivity. The overall scores are weighted averages of these values. Though these scores do roughly align with the actual sentiment of this tweet, **ALWAYS** pay attention to whats going on inside of your sentiment analysis pipeline. Even though the overall sentiment score here is negative, it should probably be even more negative; the algorithm picked up on the word \"good\" in this tweet, and this improved the polarity score by 0.7. But the context in which \"good\" was uttered in this tweet is actually negative! the person is saying \"stop saying #Tillerson is good on climate\"-- this is expressing negative sentiment!\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU1ZLDWTpK1d"
      },
      "source": [
        "### Assessed Question\n",
        "\n",
        "In this assessed question, we will use NLP to find the biggest hater.\n",
        "\n",
        "I've pulled a sample of 1000 tweets. In the code cell below:\n",
        "\n",
        "1. Using a lambda function, `.apply(lambda x: nlp(x)._.blob.polarity)`, create a column in the sample dataframe that contains the polarity of each tweet.\n",
        "2. Create a column that contains the subjectivity score for each tweet.\n",
        "3. Filter the dataframe to keep only the tweets that are subjective (subjectivity score > 0.5) *and* tweets that have negative sentiment (polarity score < 0).\n",
        "\n",
        "Which twitter user (author_id) has the lowest total sentiment?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLrKtg46pK1d"
      },
      "outputs": [],
      "source": [
        "sample= tweets.sample(1000, random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a 'polarity' column using the lambda function\n",
        "# We apply this to the 'text' column of the tweets\n",
        "sample['polarity'] = sample['text'].apply(lambda x: nlp(x)._.blob.polarity)\n",
        "\n",
        "# 2. Create a 'subjectivity' column\n",
        "sample['subjectivity'] = sample['text'].apply(lambda x: nlp(x)._.blob.subjectivity)\n",
        "\n",
        "# 3. Filter for subjective (> 0.5) AND negative (< 0) tweets\n",
        "haters_df = sample[(sample['subjectivity'] > 0.5) & (sample['polarity'] < 0)]\n",
        "\n",
        "# Optional: See what the filtered tweets look like\n",
        "print(\"--- Filtered 'Hater' Tweets (sample) ---\")\n",
        "print(haters_df[['author_id', 'text', 'polarity', 'subjectivity']].head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# 4. Find the user (author_id) with the lowest total sentiment\n",
        "# We group the filtered dataframe by author and SUM their polarities\n",
        "total_sentiment_by_author = haters_df.groupby('author_id')['polarity'].sum()\n",
        "\n",
        "# Sort the results to see who has the lowest total score\n",
        "sorted_haters = total_sentiment_by_author.sort_values(ascending=True)\n",
        "\n",
        "print(\"--- Top Haters (by total negative sentiment) ---\")\n",
        "print(sorted_haters.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Get the ID of the user with the minimum (most negative) score\n",
        "biggest_hater_id = sorted_haters.idxmin()\n",
        "lowest_score = sorted_haters.min()\n",
        "\n",
        "print(f\"The user with the lowest total sentiment (the 'biggest hater') is: {biggest_hater_id}\")\n",
        "print(f\"Their total negative sentiment score is: {lowest_score:.4f}\")"
      ],
      "metadata": {
        "id": "pGplEMgXqqKY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9C_kwLPljAY0",
        "J4P2wbX2jAZL",
        "CN8gxS-PjAZ2"
      ],
      "name": "W7. Natural Language Processing - Student Version.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('3.9.5')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "d34fbd810dd9652f8e464616181cf14dbb258b5c046bed5c2f54c6b5e518fed2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}